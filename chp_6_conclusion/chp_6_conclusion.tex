\chapter{Conclusion} \label{chap:conclusion}
%\markboth{Conclusion}{}
%\addcontentsline{toc}{chapter}{Conclusion}
\section*{Summary}
In the last decade, the multi-objective QP control paradigm has become a standard tool for controlling complex robots in task-space while mediating control objectives with different priorities and accounting explicitly for unilateral constraints. Backed up by the increasing progress in CPU computation power, real-time QP implementations have been successfully performed on a high number of robotic platforms. Despite this astonishing success, there are still open questions that are still not fully addressed by the research community. 
In this thesis, we tackled some of these open problems. 

First, QP can only handle the constraints that are expressed in terms of its decision variables. For ID-QP, the kinematic constraints cannot be directly included in QP because of their inadequate form and have to be reformulated in terms of the QP decision variables. Inspired by the PD task formulation, we proposed an ODI-based formulation with adaptive gains. This strategy allows the kinematic variables (joint position, CoM, distance to collision, etc.) to reach their boundaries exponentially and smoothly for all the initial conditions and even when the boundaries are time-varying as shown in \cref{chap:adaptive gains}. In addition, the forward invariance of the sets to which these kinematic variables belong is proved analytically. The assessed experiments on the humanoid robot HRP-4 validated the proposed approach.

The second issue we tackled in this thesis is the stability of the closed-loop QP control scheme. Instable behaviors have been so far reported in several works but never addressed explicitly. During the experiments performed in \cref{chap:adaptive gains} and others, we noticed the occurrence of instabilities described as strong and divergent oscillations and jerky motion near the boundaries of the constraints. We studied this problem by considering 1-DoF kinematic-controlled system in \cref{chap:instable qp}. Although its simplicity, this case-study allowed us to: (i) reproduce these instabilities; (ii) understand that this issue comes from the tasks and kinematic constraints gains and joint-dynamics parameters; and (iii) propose a solution to ensure stability using integral feedback and prove it using linear control theory. Nevertheless, this approach cannot be generalized to task-space. 

In \cref{chap:robust qp}, a few steps have been made to enable tackling the general case. First, to encompass a large class of robots, we considered multi-DoF kinematic-controlled robots such that their stability is assumed in Input-to-State sense. Then, we showed that the reported instability is, in fact, a result of non-robustness against the non-modeled dynamics such as the joint-dynamics, flexibilities, and external disturbances. After that, we proposed a robust PD task formulation to ensure robust stability, which we formally proved using Lyapunov theory, and a robust ODI formulation (which we denoted as RECBF) for kinematic constraints to ensure robust asymptotic stability of the corresponding set using BF theory. To show that our approach is not robot-specific, we validated our approach on two types of robots: fixed-based robotic arm Panda, and floating-base humanoid robot HRP-4.    

In \cref{chap:mpc ref gov}, we addressed another QP control problem which is the constraints compatibility. Incompatible constraints lead the QP to be infeasible because of an empty feasibility domain. Although this issue is general, we considered one case of incompatibility often encountered in practice: potential conflicts between hardware limitations and kinematic constraints formulated as ODI in \cref{chap:adaptive gains}. Dealing with compatibility in a point-wise fashion by relaxing the constraints is not a viable solution. Instead, we opted for a high-level approach based on an MPC-based reference governor layer on top of QP. MPC accounts for the closed-loop dynamics of the tasks, predicts the robot trajectories, and enforces the kinematic constraints and hardware limitations over a finite preview horizon. As a result, it outputs the optimal task targets such that the robot convergence to the reference task while satisfying the above constraints. 

After addressing these issues, we have been interested in the observation problem. This process is needed when some states are not accessible for measurements but required to perform the control. So far, performed as an exogenous process, we aimed to find a strategy to unify observation and control via a holistic QP formulation. Leveraging the multi-objective QP paradigm, our idea is to perceive the observation as a task that runs concurrently with other control tasks. In particular, we considered the case where the control tracking target is only partially known (measured), and thereby an observation task is required to construct the full-state of the target. Conversely to the classical multi-objective QP, where the tasks are independent w.r.t the targets, we proposed the novel concept of interdependent tasks where the state of the observation task is forwarded as an input for the tracking control task. In~\cref{chap:handover qp}, we showed how this concept is interesting in formulating control scenarios, especially human-robot handover. In such a case, our observer/controller formulation enables us to achieve seamless and fluid robot motion toward the handover location without knowing it in advance. By estimating the object pose, velocity and acceleration, the observation task forwards these terms as reference targets for a trajectory tracking task leading to an anticipatory motion toward the object. This approach resulted in successful handover experimentation using Panda robotic arm.  
 
\section*{Outlook}
Although the achieved contributions and developed methods in this thesis address specific issues, their generality and theoretical grounding can be used as bridges toward new research perspectives and tackle some exciting topics. 
%\begin{itemize}
%	\item \cref{chap:robust qp}: 
%	\begin{itemize}
%	%	\item 
%		\subsection*{Conservativeness} 
%		\begin{itemize}
%	%		\item \cref{thm:heterogeneous feedback,thm:RECBF} do only prove the existence of  integral gains that ensure robust stability. A constructive method to tune these gains is still missing. This would lower the conservativeness of the proposed approach in \cref{chap:robust qp}.
%			\item In~\cref{chap:mpc ref gov}, the task and constraint Jacobians are assumed to remain constant during the preview horizon. This is, in fact, a rough approximation of the system dynamics. Enhancing the MPC with a Jacobians prediction scheme will considerably lower the conservativeness and improve the prediction precision. A hierarchical architecture proposed in~\cite{li2021ral} can also be a good compromise between prediction accuracy and computation simplicity.
%		\end{itemize}
		%\item 
		\subsection*{Compliance Generation} According to the proposed closed-loop scheme in~\cref{fig:whole control scheme}, the effect of the external disturbance is counterbalanced by QP. In some cases,  we would like the robot to be compliant with the external disturbance, especially if it is due to unforeseen contact. However, the compliant behavior needs to be generated by having a kinematic-controlled robot. Building on top of the proposed feedback~\cref{eq:heterogeneous feedback mu} and exploiting the  F/T sensors, a control strategy can be drawn to achieve this purpose. For instance, adding the double integral feedback term $\mathbf{K}^{\fkin}_{\rm ii}\desTaskOut_1$ with a sophisticated tuning of the gains $\mathbf{K}^{\fkin}_{\rm ii}$ can be a good starting point to explore this feature. 
%	\end{itemize} 
	%\item 
%	\subsection*{Passivity}
%	Passivity is a promising framework to study the stability of a system subject to an input by monitoring the energy flow. This framework has been so far applied to ensure stability of torque-controlled robots under external disturbances ensuring thereby safe interaction. From the proof of \cref{thm:heterogeneous feedback}
	\subsection*{Task Scheduling} 
	Classically, task sequencing is handled by either introducing/removing tasks on-the-fly, or via FSM. However, this would generally result in a discontinuous joint acceleration and torque and a typically flawless hashed motion. Explicit time optimization of the sequencing of the tasks has been proposed in~\cite{keith2009iros}. Nevertheless, an ordered sequence of tasks has to be defined beforehand. Alternatively, we can handle task scheduling by defining constraints on the convergence errors of these tasks exploiting the ODI formulation proposed in \cref{chap:adaptive gains}. Namely, we can imagine the case where two tasks have the same priority in terms of weight (solution predominance). Yet, one task must converge before the other. We can even enforce this behavior by imposing the same constraint on the predicted tasks convergence errors using MPC proposed in~\cref{chap:mpc ref gov}.
	%achieved via FSM: the next task is triggered once the completion criteria of the precedent one is reached. However, this is can be tricky in complex scenarios where many states have to be defined, in addition to the  resulting motion that is potentially hashed and discontinuous. Explicit time optimization of the tasks sequencing has been proposed in~\cite{keith2009iros}. Nevertheless, a clear ordered sequence of tasks has to be defined beforehand. Alternatively, we can handle tasks scheduling by defining constraints on the  convergence errors of these tasks exploiting the ODI formulation proposed in \cref{chap:adaptive gains} with $\bfunc_{1,2} = \norm{\actOut^{\fkin_1} - \actOut^{\fkin_2}}$.  This method has three major benefits: (i) the versatility, e.g., scaling the task errors; (ii) straightforwardly generalized to $n$ tasks, (ii) no offline computation is needed. 
%	An other research direction is task scheduling. Classically, the tasks timing is handled by either  introducing/removing tasks on-the-fly, or executed sequentially by a FSM. However, this would generally results in a discontinuous joint acceleration and torque, and a typically flawless  hashed motion. We can think of handling task scheduling through MPC by imposing constraints on their convergence. Namely, we can imagine the case where two tasks have the same priority in terms of weight (solution predominance), yet one task must converge before the other one.
%	\begin{itemize}
	%\item 
	\subsection*{Task Gains Adaptation via QP} 
	The task gains can be included in the QP decision variables while still having a least-square form as in~\cref{eq-chap0:least-square general form}. This approach would adapt online gains depending on the concurrent tasks and active constraints. The performance of some tasks (e.g., admittance) is very sensitive to the chosen gains. Adaptive gains QP can be a nice solution to mitigate such issues.
	\subsection*{Feasibility Prediction}
In \cref{chap:mpc ref gov}, we only considered the compatibility between hardware limitations and kinematic constraints. In~\cite{morris2013cdc}, a metric of the feasibility domain width is proposed and interpreted as the distance to the closest inequality. Designing an MPC that predicts the feasibility of QP along a finite preview horizon can constitute a breakthrough for tackling this issue in a more general and efficient way.
	\subsection*{Teleoperation}
	When controlling a robot in teleoperation, synchronization between the operation motion and robot execution is very important. In fact, the observation task proposed in~\cref{chap:handover qp} can be used to estimate the full-state of the frame attached to the human. The estimated states enable us to proactively mimic human actions without explicitly predicting the human gestures.   
%	\end{itemize}
%\end{itemize} 
